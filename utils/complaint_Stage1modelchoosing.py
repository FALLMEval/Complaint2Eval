import numpy as np
import pandas as pd
from sentence_transformers import SentenceTransformer, util

# Load embedding model
embedder = SentenceTransformer("all-mpnet-base-v2")


def calculate_set_similarity(sentences):
    """
    Compute the average pairwise cosine similarity among a set of sentences.
    Returns NaN if fewer than 2 sentences are available.
    """
    if len(sentences) <= 1:
        return np.nan
    embeddings = embedder.encode(sentences, convert_to_tensor=True)
    sim_matrix = util.cos_sim(embeddings, embeddings).cpu().numpy()
    n = len(sentences)
    sims = [sim_matrix[i][j] for i in range(n) for j in range(i + 1, n)]
    return float(np.mean(sims))


def analyze_model_similarity(results):
    """
    Analyze similarity of questions generated by each model

    Args:
        results: Output from process_complaints_batch function

    Returns:
        df_summary: Model-level summary
        df_casewise: Case-level details
        chosen_model: The automatically chosen best model
    """

    # Step 1: Build case-level records with number of questions and similarity score
    case_records = []

    for item in results:
        # Skip error cases
        if "error" in item or "parse_error" in item:
            continue

        model = item.get("Model")
        index = item.get("Complaint_Index")

        # Extract questions from the parsed JSON
        # Check different possible structures in your data
        questions = []

        if "evaluation_criteria" in item:
            # If your data has evaluation_criteria structure
            criteria = item.get("evaluation_criteria", [])
            if isinstance(criteria, list):
                questions = [
                    q.get("question", "")
                    for q in criteria
                    if isinstance(q, dict) and "question" in q
                ]
            elif isinstance(criteria, dict):
                # Sometimes it might be a single dict
                questions = [criteria.get("question", "")]
        elif "questions" in item:
            # If your data has a questions field
            questions = item.get("questions", [])
        else:
            # Extract all string values that look like questions
            for key, value in item.items():
                if key not in [
                    "Model",
                    "Complaint_Index",
                    "Timestamp",
                    "raw_response",
                    "parse_error",
                    "error",
                ]:
                    if (
                        isinstance(value, str) and len(value) > 10
                    ):  # Assume questions are longer than 10 chars
                        questions.append(value)
                    elif isinstance(value, list):
                        # Check if it's a list of question objects
                        for v in value:
                            if isinstance(v, dict) and "question" in v:
                                questions.append(v["question"])
                            elif isinstance(v, str) and len(v) > 10:
                                questions.append(v)

        # Filter out empty questions
        questions = [q for q in questions if q and q.strip()]

        if not questions:
            # Skip this case and continue with others
            continue

        case_records.append(
            {
                "Model": model,
                "Complaint_Index": index,
                "Num_Questions": len(questions),
                "SimilarityScore": calculate_set_similarity(questions),
                "Questions": questions,  # Keep questions for reference
            }
        )

    if not case_records:
        print("No valid case records found!")
        return None, None, None

    df_casewise = pd.DataFrame(case_records)

    print(f"Analyzed {len(case_records)} cases")
    print(f"Models: {df_casewise['Model'].unique()}")

    # Step 2: Build model-level summary (average values across all cases)
    df_summary = df_casewise.groupby("Model", as_index=False).agg(
        {"Num_Questions": ["mean", "std", "count"], "SimilarityScore": ["mean", "std"]}
    )

    df_summary.columns = [
        "Model",
        "Avg_Num_Questions",
        "Std_Num_Questions",
        "Case_Count",
        "Avg_SimilarityScore",
        "Std_SimilarityScore",
    ]

    numeric_cols = [
        "Avg_Num_Questions",
        "Std_Num_Questions",
        "Avg_SimilarityScore",
        "Std_SimilarityScore",
    ]
    df_summary[numeric_cols] = df_summary[numeric_cols].round(3)

    # Choose the best model based on your criteria
    df_filtered = df_summary.copy()

    # Exclude models with case count below the mode (most common case count)
    if not df_filtered.empty:
        mode_case_count = df_filtered["Case_Count"].mode()[0]

        df_filtered = df_filtered[df_filtered["Case_Count"] >= mode_case_count]

        low_case_models = df_filtered[df_filtered["Case_Count"] < mode_case_count][
            "Model"
        ].tolist()
        if low_case_models:
            print(
                f"Models excluded due to case count below mode ({mode_case_count}): {low_case_models}"
            )

    if df_filtered.empty:
        print("Warning: All models were excluded from analysis")
        chosen_model = None
    else:
        mean_val = df_filtered["Avg_Num_Questions"].mean()
        std_val = df_filtered["Avg_Num_Questions"].std()
        print(f"Mean Avg_Num_Questions: {mean_val:.2f}")
        print(f"Standard Deviation: {std_val:.2f}")

        # Filter models within 1 standard deviation of mean
        lower_bound = mean_val - std_val
        upper_bound = mean_val + std_val
        df_in_range = df_filtered[
            (df_filtered["Avg_Num_Questions"] >= lower_bound)
            & (df_filtered["Avg_Num_Questions"] <= upper_bound)
        ]

        if df_in_range.empty:
            print("Warning: No models found within 1 standard deviation of mean")
            print(
                "Selecting model with lowest similarity score from all filtered models"
            )
            chosen_model = df_filtered.sort_values("Avg_SimilarityScore").iloc[0][
                "Model"
            ]
            chosen_model_stats = df_filtered.sort_values("Avg_SimilarityScore").iloc[0]
        else:
            chosen_model = df_in_range.sort_values("Avg_SimilarityScore").iloc[0][
                "Model"
            ]
            chosen_model_stats = df_in_range.sort_values("Avg_SimilarityScore").iloc[0]

        print(f"\n=== Model Selection Process ===")
        print(f"Models considered: {list(df_filtered['Model'].values)}")
        print(f"Model chosen: {chosen_model}")
        print(f"  - Avg Questions: {chosen_model_stats['Avg_Num_Questions']:.2f})")
        print(
            f"  - Similarity Score: {chosen_model_stats['Avg_SimilarityScore']:.3f} (lower = more diverse)"
        )
        print(f"  - Cases Processed: {chosen_model_stats['Case_Count']}")

    return df_summary, df_casewise, chosen_model
